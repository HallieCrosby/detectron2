{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download, decompress the data\n",
    "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip \n",
    "!unzip balloon_dataset.zip\n",
    "!rm -r __MACOSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sagemaker.Session() # can use LocalSession() to run container locally\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "region = \"us-east-2\"\n",
    "prefix_input = 'detectron2-input'\n",
    "prefix_output = 'detectron2-ouput'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need to upload it to S3\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=region)\n",
    "root_path = os.getcwd()\n",
    "data_path = os.path.join(root_path, \"balloon\")\n",
    "\n",
    "for path, subdirs, files in os.walk(data_path):\n",
    "    directory_name = path.replace(root_path+\"/\",\"\")\n",
    "    for file in files:\n",
    "        #print(os.path.join(root_path, directory_name, file))\n",
    "        s3_resource.Bucket(bucket).upload_file(os.path.join(root_path, directory_name, file), directory_name+'/'+file)\n",
    "        #print(f\"file {file} has been uploaded to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "Let's review the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.utils.logger\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m setup_logger\n",
      "setup_logger()\n",
      "\n",
      "\u001b[37m# import some common libraries\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcv2\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "    \n",
      "\u001b[37m# install pycocotools\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mgit+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \n",
      "\u001b[37m# import some common detectron2 utilities\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m model_zoo\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.engine\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DefaultPredictor\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.config\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_cfg\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.utils.visualizer\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Visualizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.data\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MetadataCatalog\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.engine\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DefaultTrainer\n",
      "\n",
      "\n",
      "\u001b[37m# packages neededs for custom dataset\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.structures\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BoxMode\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.data\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DatasetCatalog\n",
      "    \n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m():\n",
      "    cfg = get_cfg()\n",
      "    cfg.merge_from_file(model_zoo.get_config_file(\u001b[33m\"\u001b[39;49;00m\u001b[33mCOCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    cfg.DATASETS.TRAIN = (\u001b[33m\"\u001b[39;49;00m\u001b[33mballoon_train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,)\n",
      "    cfg.DATASETS.TEST = ()\n",
      "    cfg.DATALOADER.NUM_WORKERS = \u001b[34m2\u001b[39;49;00m\n",
      "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\u001b[33m\"\u001b[39;49;00m\u001b[33mCOCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)  \u001b[37m# Let training initialize from model zoo\u001b[39;49;00m\n",
      "    cfg.SOLVER.IMS_PER_BATCH = \u001b[34m2\u001b[39;49;00m\n",
      "    cfg.SOLVER.BASE_LR = \u001b[34m0.00025\u001b[39;49;00m  \u001b[37m# pick a good LR\u001b[39;49;00m\n",
      "    cfg.SOLVER.MAX_ITER = \u001b[34m300\u001b[39;49;00m    \u001b[37m# 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\u001b[39;49;00m\n",
      "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = \u001b[34m128\u001b[39;49;00m   \u001b[37m# faster, and good enough for this toy dataset (default: 512)\u001b[39;49;00m\n",
      "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = \u001b[34m1\u001b[39;49;00m  \u001b[37m# only has one class (ballon)\u001b[39;49;00m\n",
      "    cfg.OUTPUT_DIR = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] \u001b[37m# TODO check that this config works fine\u001b[39;49;00m\n",
      "\n",
      "    trainer = DefaultTrainer(cfg) \n",
      "    trainer.resume_or_load(resume=\u001b[36mFalse\u001b[39;49;00m)\n",
      "    trainer.train()\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprepare_dataset\u001b[39;49;00m():\n",
      "    \u001b[34mfor\u001b[39;49;00m d \u001b[35min\u001b[39;49;00m [\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]:\n",
      "        DatasetCatalog.register(\u001b[33m\"\u001b[39;49;00m\u001b[33mballoon_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + d, \u001b[34mlambda\u001b[39;49;00m d=d: get_balloon_dicts(os.environ[f\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_{d.upper()}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\n",
      "        MetadataCatalog.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mballoon_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + d).set(thing_classes=[\u001b[33m\"\u001b[39;49;00m\u001b[33mballoon\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_balloon_dicts\u001b[39;49;00m(img_dir):\n",
      "    \n",
      "    json_file = os.path.join(img_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mvia_region_data.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(json_file) \u001b[34mas\u001b[39;49;00m f:\n",
      "        imgs_anns = json.load(f)\n",
      "\n",
      "    dataset_dicts = []\n",
      "    \u001b[34mfor\u001b[39;49;00m idx, v \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(imgs_anns.values()):\n",
      "        record = {}\n",
      "        \n",
      "        filename = os.path.join(img_dir, v[\u001b[33m\"\u001b[39;49;00m\u001b[33mfilename\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "        height, width = cv2.imread(filename).shape[:\u001b[34m2\u001b[39;49;00m]\n",
      "        \n",
      "        record[\u001b[33m\"\u001b[39;49;00m\u001b[33mfile_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = filename\n",
      "        record[\u001b[33m\"\u001b[39;49;00m\u001b[33mimage_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = idx\n",
      "        record[\u001b[33m\"\u001b[39;49;00m\u001b[33mheight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = height\n",
      "        record[\u001b[33m\"\u001b[39;49;00m\u001b[33mwidth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = width\n",
      "      \n",
      "        annos = v[\u001b[33m\"\u001b[39;49;00m\u001b[33mregions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        objs = []\n",
      "        \u001b[34mfor\u001b[39;49;00m _, anno \u001b[35min\u001b[39;49;00m annos.items():\n",
      "            \u001b[34massert\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m anno[\u001b[33m\"\u001b[39;49;00m\u001b[33mregion_attributes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "            anno = anno[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape_attributes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "            px = anno[\u001b[33m\"\u001b[39;49;00m\u001b[33mall_points_x\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "            py = anno[\u001b[33m\"\u001b[39;49;00m\u001b[33mall_points_y\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "            poly = [(x + \u001b[34m0.5\u001b[39;49;00m, y + \u001b[34m0.5\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m x, y \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(px, py)]\n",
      "            poly = [p \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m poly \u001b[34mfor\u001b[39;49;00m p \u001b[35min\u001b[39;49;00m x]\n",
      "\n",
      "            obj = {\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mbbox\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mbbox_mode\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: BoxMode.XYXY_ABS,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33msegmentation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: [poly],\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mcategory_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33miscrowd\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m\n",
      "            }\n",
      "            objs.append(obj)\n",
      "        record[\u001b[33m\"\u001b[39;49;00m\u001b[33mannotations\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = objs\n",
      "        dataset_dicts.append(record)\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataset_dicts\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    \n",
      "    prepare_dataset()\n",
      "    train()\n"
     ]
    }
   ],
   "source": [
    "! pygmentize d2_script/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-07 16:44:47 Starting - Starting the training job...\n",
      "2020-04-07 16:44:48 Starting - Launching requested ML instances...\n",
      "2020-04-07 16:45:46 Starting - Preparing the instances for training.........\n",
      "2020-04-07 16:47:07 Downloading - Downloading input data\n",
      "2020-04-07 16:47:07 Training - Downloading the training image.........\n",
      "2020-04-07 16:48:44 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-04-07 16:48:45,375 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-04-07 16:48:45,399 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-04-07 16:48:48,425 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-04-07 16:48:48,717 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-04-07 16:48:48,717 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-04-07 16:48:48,717 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-04-07 16:48:48,718 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmp2z7n4yuq/module_dir\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/facebookresearch/detectron2.git (from -r requirements.txt (line 5))\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-utq3pygj\n",
      "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-utq3pygj\u001b[0m\n",
      "\u001b[34mCollecting termcolor>=1.1\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (7.1.0)\u001b[0m\n",
      "\u001b[34mCollecting yacs>=0.1.6\n",
      "  Downloading yacs-0.1.6-py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (4.42.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\n",
      "  Downloading tensorboard-2.2.0-py3-none-any.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting fvcore\n",
      "  Downloading fvcore-0.1.dev200407.tar.gz (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (0.17.1)\u001b[0m\n",
      "\u001b[34mCollecting pydot\n",
      "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from yacs>=0.1.6->detectron2==0.1.1->-r requirements.txt (line 5)) (5.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (2.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (0.34.2)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (46.1.3.post20200330)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (3.11.3)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.13.1-py2.py3-none-any.whl (87 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (1.14.0)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.28.1-cp36-cp36m-manylinux2010_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting portalocker\n",
      "  Downloading portalocker-1.6.0-py2.py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (1.25.8)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (3.4.2)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (0.4.8)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, detectron2, termcolor, fvcore, absl-py\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=8447 sha256=abce8ab250efb32892b96fa77de30839887ffaa2a8a9e6d7e9ddcfd11ba1b936\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-povyfem9/wheels/67/db/01/b2ec85c3d6cafedfecf0215a9c6c5e8dc22c060f750de2e210\n",
      "  Building wheel for detectron2 (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "d2 = PyTorch(entry_point=\"train.py\", source_dir=\"d2_script\",\n",
    "             train_instance_count=1,\n",
    "             role=role,\n",
    "             train_instance_type='ml.p3.2xlarge',\n",
    "             framework_version=\"1.4.0\",\n",
    "             debugger_hook_config=False)\n",
    "\n",
    "#d2.set_hyperparameters(num_epochs = 1, num_classes = 2, )\n",
    "\n",
    "d2.fit({'train':\"s3://sagemaker-us-east-2-553020858742/balloon/train\",\n",
    "        'val':\"s3://sagemaker-us-east-2-553020858742/balloon/val\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
